{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec36e089",
   "metadata": {},
   "source": [
    "# Mouth noise detection and remediation\n",
    "## Problem overview\n",
    "The focus of this workbook is to identify mouth noises from spoken audio, because touching these up manually is a real pain. These are just the minor clicks your mouth generates when saliva catches on various parts of your mouth as you speak, it's not really noticeable in day to day conversation, but really sticks out in podcasts/voiceover recordings. Somehow, I could not find any good tools for automatically doing this online, which is really surprising to me, everything I could find was about removing general background noise, not these minor clicks.\n",
    "\n",
    "From experience, I can usually tell these just from an eyeball of the waveform and spectrogram, these seems to be reliable cues for when it’s occurring - generally an overlaid frequency spike with a duration of between 8 and 60 (median ~25) ten-thousandths of a second, particularly common on “a” sounds, at the end of a sentence, or before “the” sounds (although can appear in many other locations).\n",
    "\n",
    "The solution for these (once identified) in Audacity is to use the “repair” tool over the specified section (or, since the section commonly is greater than the 128 sample limit, repeatedly using it over the duration) to “smooth” the soundwave of that section based on the brief sample before and after it. Alternatively, just cut the section entirely, picking points before and after with equal positions on the waveform - because these sounds are so short, it is almost impossible to notice their cutting.\n",
    "\n",
    "I recorded a ~22 minute piece of sample audio, where I intentionally didn’t use any of the “just be a virtuous person” bullshit “mouth noise reducing tips” espoused on all the Google search results. So far I have gone through the first 6:36 of the recording and manually annotated the start and rough end times of 464 mouth sounds, which is where I got the duration values above. I *was* going to train an ML tool on this dataset, but it turned out my naive benchmark below did the job just fine.\n",
    "\n",
    "(I have a separate private notebook where these is a bunch of me screwing around figuring out how to work with sound files/what fourier transform functions actually do, etc. I've cleared all that out for this public version, but if you want to improve on it, I'd suggest diving into the librosa library and gettign a feel for all that stuff).\n",
    "\n",
    "## Naïve approach\n",
    "\n",
    "This is done by loading a soundfile through librosa, cutting it into 4ms chunks each starting 1ms after the other, and then analysing the average frequency of the chunk. If a given chunk has a substantially greater average frequency to the ones abutting it (e.g mean frequency is >6000hz and >3x mean frequency of chunk ending the sample prior to it and chunk starting the sample after it), it will be flagged as a likely click.\n",
    "\n",
    "Once the entire recording has been searched and chunks classified, overlapping chunks will be analysed to determine the highest frequency chunk, and this will be selected as the definitive \"mouth noise\" chunk for that section.\n",
    "\n",
    "For this to work, you need a folder in the same directory as this notebook called \"WAV\", and save your file to be analysed in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1feea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time \n",
    "\n",
    "# Set a variable for the current notebook's path for various loading/saving mechanisms\n",
    "nb_path = os.getcwd()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def gross_mag_in_threshold(chunk, sr=44100, band_start=5000, band_end=10000):\n",
    "    \"\"\"Calculate the total intensity of frequencies within a chunk using Fourier Transform.\n",
    "    chunk - array of longs - a librosa loaded sound object containing sound data to be analyzed\n",
    "    band_start - int - the minimum frequency band to include\n",
    "    band_end - int - the maximum frequency band to include\n",
    "    \"\"\"\n",
    "    stft = librosa.stft(chunk, n_fft=len(chunk))\n",
    "    \n",
    "    # Get the frequency bin indices\n",
    "    freq_bins = librosa.fft_frequencies(sr=sr, n_fft=len(chunk))\n",
    "\n",
    "    # Get the magnitude of each frequency band by averaging each element returned in stft\n",
    "    magnit = np.abs(stft)\n",
    "    av_mag = np.mean(magnit, axis=1)\n",
    "    gross_mag = np.sum(av_mag * (freq_bins > band_start) * (freq_bins < band_end))\n",
    "    return gross_mag\n",
    "\n",
    "\n",
    "def tick_detector(chunks, start_times, chunk_ms=4, threshold_factor=3, band_start=5000, band_end=10000, sr=44100):\n",
    "    \"\"\"Returns a pandas DataFrame presenting timestamps within a sound file that contain ticks.\n",
    "    chunks - array of overlapping librosa loaded sound objects containing sound data to be analyzed\n",
    "    start_times - list of longs - timestamps (in seconds) of the time where each chunk begins\n",
    "    chunk_ms - length (in milliseconds) of each chunk\n",
    "    threshold_factor - long - how much a chunk has to exceed its neighbors by to indicate a likely tick\n",
    "    band_start - int - the minimum frequency band to include\n",
    "    band_end - int - the maximum frequency band to include\n",
    "    sr - int - sample rate of the chunks\n",
    "    \"\"\"\n",
    "    flags = np.zeros(len(chunks), dtype=bool) #initialize array with all False values\n",
    "    \n",
    "    chunk_size = int(sr * (chunk_ms / 1000))  # chunk size in samples\n",
    "\n",
    "    chunk_mags = np.array([gross_mag_in_threshold(chunk, band_start=band_start, band_end=band_end, sr=sr)\n",
    "                                   for chunk in chunks])\n",
    "    \n",
    "    # set array for comparable chunks (e.g. chunk n can only be compared to n-4 and n+4)\n",
    "    prev_chunks = chunks[:-(2 * chunk_ms)]\n",
    "    current_chunks = chunks[chunk_ms:-chunk_ms]\n",
    "    next_chunks = chunks[(2 * chunk_ms):]\n",
    "\n",
    "    prev_frequency_mag = chunk_mags[:-(2 * chunk_ms)]\n",
    "    current_frequency_mag = chunk_mags[chunk_ms:-chunk_ms]\n",
    "    next_frequency_mag = chunk_mags[(2 * chunk_ms):]\n",
    "    \n",
    "    flags[chunk_ms:-chunk_ms] = (current_frequency_mag > threshold_factor * prev_frequency_mag) & \\\n",
    "                                (current_frequency_mag > threshold_factor * next_frequency_mag)\n",
    "\n",
    "    df = pd.DataFrame({'Start Time': start_times, 'Flag': flags})\n",
    "    \n",
    "    # Filter the flagged chunks\n",
    "    mouth_sounds = df.loc[df['Flag'] == True].copy()\n",
    "    # Calculate the end times - need to use chunk_size because of rounding issues\n",
    "    mouth_sounds['End Time'] = mouth_sounds['Start Time'] + (chunk_size / sr)\n",
    "    # Reset the index\n",
    "    mouth_sounds.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return mouth_sounds\n",
    "\n",
    "\n",
    "def generate_mouth_sounds_labels(filepath, output_file_name = 'output_file.txt', sample_rate=None):\n",
    "    kickoff_time = time.time()\n",
    "    print(time.strftime('%X %x %Z'))\n",
    "\n",
    "    test1, sr = librosa.load(filepath, sr=sample_rate)\n",
    "    \n",
    "    # cutting it into chunks of \"chunk_ms\" length each starting 1ms after the other\n",
    "    chunk_ms = 4\n",
    "    chunk_size = int(sr * (chunk_ms / 1000))\n",
    "    hop_size = int(sr * 0.001)\n",
    "    \n",
    "    chunks = np.array([test1[i:i+chunk_size] for i in range(0, len(test1) - chunk_size, hop_size)])\n",
    "    start_times = np.arange(0, len(chunks) * hop_size / sr, hop_size / sr)\n",
    "\n",
    "    df_2 = tick_detector(chunks, start_times, threshold_factor=2, band_end=12000, sr=sr)\n",
    "    \n",
    "    # Initialize the consolidated dataframe\n",
    "    consolidated_chunks = pd.DataFrame(columns=['Start Time', 'End Time'])\n",
    "    \n",
    "    # Initialize variables for tracking the current chunk\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "\n",
    "    # Iterate through each row in the dataframe\n",
    "    for index, row in df_2.iterrows():\n",
    "        if current_start is None:\n",
    "            # If it's the first chunk, set the current chunk\n",
    "            current_start = row['Start Time']\n",
    "            current_end = row['End Time']\n",
    "        elif row['Start Time'] <= current_end:\n",
    "            # If the current chunk overlaps with the next chunk, extend the current chunk\n",
    "            current_end = row['End Time']\n",
    "        else:\n",
    "            # If the next chunk is not overlapping, add the consolidated chunk to the dataframe\n",
    "            consolidated_chunks = consolidated_chunks.append({'Start Time': current_start, 'End Time': current_end},\n",
    "                                                             ignore_index=True)\n",
    "            # Set the next chunk as the current chunk\n",
    "            current_start = row['Start Time']\n",
    "            current_end = row['End Time']\n",
    "\n",
    "    # Add the last consolidated chunk to the dataframe\n",
    "    consolidated_chunks = consolidated_chunks.append({'Start Time': current_start, 'End Time': current_end},\n",
    "                                                     ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # iterate over each chunk 4 steps each delayed 0.5ms to find the maximum energy in high frequency bands to narrow\n",
    "    # down the starting point. This part is ripe for optimisation. I had 1 attempt but it didn't really work out.\n",
    "    \n",
    "    # Iterate over each row in the dataframe\n",
    "    for index, row in consolidated_chunks.iterrows():\n",
    "        start_time = row['Start Time']\n",
    "        end_time = row['End Time']\n",
    "\n",
    "        # Find the corresponding samples for the start and end times\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "\n",
    "        # Extract the chunk from the original loaded .wav object\n",
    "        chunk = test1[start_sample:end_sample]\n",
    "\n",
    "        # Create four new chunks with staggered start times (0.5ms apart)\n",
    "        chunk_offsets = [0, int(0.5e-3 * sr), int(1e-3 * sr), int(1.5e-3 * sr)]\n",
    "        chunks = [chunk[offset:] for offset in chunk_offsets]\n",
    "\n",
    "        # Calculate the energy for each chunk within the desired frequency band\n",
    "        energies = [gross_mag_in_threshold(chunk, sr=sr) for chunk in chunks]\n",
    "\n",
    "        # Find the index of the chunk with the maximum energy\n",
    "        max_energy_index = max(0, np.argmax(energies)-1)\n",
    "\n",
    "        # Update the start time based on the selected chunk\n",
    "        new_start_time = start_time + chunk_offsets[max_energy_index] / sr\n",
    "\n",
    "        # Update the dataframe with the new start time\n",
    "        consolidated_chunks.loc[index, 'Start Time'] = new_start_time\n",
    "    \n",
    "    #export the dataframe to a tab delimited text file for import into Audacity\n",
    "    consolidated_chunks['Index'] = consolidated_chunks.index\n",
    "    \n",
    "    consolidated_chunks.to_csv(output_file_name, sep='\\t', columns=['Start Time', 'End Time', 'Index'], index=False)\n",
    "\n",
    "    print(time.strftime('%X %x %Z'))\n",
    "    print(\"time taken: --- %s seconds ---\" % (time.time() - kickoff_time))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea41ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:42:55 06/27/23 AUS Eastern Standard Time\n",
      "10:43:10 06/27/23 AUS Eastern Standard Time\n",
      "time taken: --- 14.295339584350586 seconds ---\n"
     ]
    }
   ],
   "source": [
    "generate_mouth_sounds_labels('{}\\WAV\\\\test 30s.wav'.format(nb_path), \"30s44100.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b772e088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:44:36 06/27/23 AUS Eastern Standard Time\n",
      "10:44:49 06/27/23 AUS Eastern Standard Time\n",
      "time taken: --- 12.711917638778687 seconds ---\n"
     ]
    }
   ],
   "source": [
    "generate_mouth_sounds_labels('{}\\WAV\\\\test 30s.wav'.format(nb_path), \"30s22050Hz.txt\", sample_rate= 22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093cc53",
   "metadata": {},
   "source": [
    "I believe STFT limits frequency bands to n/2 of sample rate, so would not want to go below 22050. Smaller sample rate is slightly faster, but identifies 9 fewer mouth noises (not sure if any of these are false positives in the 44100 sr output).\n",
    "   \n",
    "Further opportunities:   \n",
    " - Set up the loaded sound file as a class.\n",
    " - Optimise the \"fine tuning\" section of \"generate_mouth_sounds_labels()\" to use vectors instead of loops.\n",
    " - Extend the \"fine tuning\" section to trim the tail of the identified clip, not just the lead.\n",
    " - Multithreading. Since the \"process\" is comparing a set of independent chunks, it would be very possible to divide the total sound file into n threads and simultaneously process each thread. Potential bottleneck on memory?\n",
    " - Add a \"clean\" function which duplicates the mechanism of the \"repair\" function in Audacity to re-write the identified \"mouthsound\" datapoints with a smoothed interpolated function and export a new sound file. This way all processing is done in one space rather than flitting between programs.\n",
    " - Write this as a .py file that can be used via terminal instead of a notebook.\n",
    " - Be less lazy.\n",
    " - Write up a summary of the mechanics of all this so someone can learn and actually implement these improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2232a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
