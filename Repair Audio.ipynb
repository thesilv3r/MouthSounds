{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbf56b3",
   "metadata": {},
   "source": [
    "# Audio Repair\n",
    "Workbook for replicating the functionality of Audacity to repair audio sections based on interpolation of adjacent buffered inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a3fa90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv\n",
    "import soundfile as sf\n",
    "import os\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a variable for current notebook's path for various loading/saving mechanisms\n",
    "nb_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489cd49c",
   "metadata": {},
   "source": [
    "Below, I'm tempted to replace the \"linear_interpolate_audio\" function with a cosine interpolation function. Since the chunks are extremely small, I expect filling them with a smooth curve will hopefully have minimal impact? I've generally found that this is the typical result of Audactiy's \"repair\" function, and that alternately simply deleting chunks from the start of the click up to a sample with equal amplitude on a successive wave also usually results in clean audio. There is the opportunity to fill the space with a cosine function. \n",
    "\n",
    "$ {amplitude \\over 2} \\cdot cos({duration \\over π }) + c $\n",
    "\n",
    "amplitude: the sample value before the start of the chunk minus the sample value after.  \n",
    "duration: the length of the chunk in samples.   \n",
    "c: the mean of the sample value before and after the chunk.\n",
    "\n",
    "This way, the space will always be filled with a smooth curve of samples for its duration, which will also reflect its direction (being a positive or negative curve), size and length. Because the cosine starts at the peak (or trough) of a wave, it will smoothly transition to the other end of the filled section, and by adjusting the length with π, it will always end at a trough (or peak) as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190c3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolate_audio(buffer, first_bad, num_bad):\n",
    "    '''\n",
    "    Output: This function modifies the buffer array in-place by performing linear \n",
    "    interpolation on the specified range of bad samples. After the function is executed, \n",
    "    the bad samples in the buffer will be replaced with interpolated values.\n",
    "    \n",
    "    buffer: The array of sound samples.\n",
    "    first_bad: The index of the first bad sample in the buffer.\n",
    "    num_bad: The number of consecutive bad samples starting from first_bad.\n",
    "    '''\n",
    "    \n",
    "    decay = 0.9\n",
    "\n",
    "    if first_bad == 0:\n",
    "        # If there are no samples before the bad section, take a linear function from the last \"bad\" sample \n",
    "        # and the one immediately following\n",
    "        delta = buffer[num_bad] - buffer[num_bad + 1]\n",
    "        value = buffer[num_bad]\n",
    "        i = num_bad - 1\n",
    "        while i >= 0:\n",
    "            value += delta\n",
    "            buffer[i] = value\n",
    "            value *= decay\n",
    "            delta *= decay\n",
    "            i -= 1\n",
    "    elif first_bad + num_bad == len(buffer):\n",
    "        delta = buffer[first_bad - 1] - buffer[first_bad - 2]\n",
    "        value = buffer[first_bad - 1]\n",
    "        i = first_bad\n",
    "        while i < first_bad + num_bad:\n",
    "            value += delta\n",
    "            buffer[i] = value\n",
    "            value *= decay\n",
    "            delta *= decay\n",
    "            i += 1\n",
    "    else:\n",
    "        v1 = buffer[first_bad - 1]\n",
    "        v2 = buffer[first_bad + num_bad]\n",
    "        value = v1\n",
    "        delta = (v2 - v1) / (num_bad + 1)\n",
    "        i = first_bad\n",
    "        while i < first_bad + num_bad:\n",
    "            value += delta\n",
    "            buffer[i] = value\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "485a0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation from C to Python of Audactiy Interpolation function\n",
    "def interpolate_audio(buffer, first_bad, num_bad):\n",
    "    '''\n",
    "    Output: This function modifies the buffer array in-place by performing interpolation \n",
    "    using the LSAR (Least Squares AutoRegression) algorithm. The bad samples in the specified \n",
    "    range will be replaced with interpolated values based on the surrounding audio data. \n",
    "    If the LSAR algorithm fails (due to a singular matrix or other conditions), it falls back \n",
    "    to using linear_interpolate_audio to perform linear interpolation.\n",
    "    \n",
    "    buffer: The array of sound samples.\n",
    "    first_bad: The index of the first bad sample in the buffer.\n",
    "    num_bad: The number of consecutive bad samples starting from first_bad.\n",
    "\n",
    "    '''\n",
    "\n",
    "    len_buffer = len(buffer)\n",
    "    \n",
    "    if num_bad >= len_buffer:\n",
    "        return  # Should never have been called!\n",
    "\n",
    "    if first_bad == 0:\n",
    "        # Reverse the buffer and interpolate in the reversed order\n",
    "        reversed_buffer = np.flip(buffer)\n",
    "        interpolate_audio(reversed_buffer, len_buffer - first_bad, num_bad)\n",
    "        buffer[:] = np.flip(reversed_buffer)\n",
    "        return\n",
    "\n",
    "    s = np.array(buffer)\n",
    "\n",
    "    # Define the order of the autoregression calculation\n",
    "    # (The order of an autoregression is the number of immediately preceding values \n",
    "    # in the series that are used to predict the value at the present time).\n",
    "    # Determined based on number of bad samples and available data on either \n",
    "    # side of the section.\n",
    "    # Generally prefer a smaller window for simplicity.\n",
    "    P = min(min(num_bad * 3, 50), max(first_bad - 1, len_buffer - (first_bad + num_bad) - 1))\n",
    "\n",
    "    # If the length of the bad sample is 1 or the window the size of the available array, \n",
    "    # use linear interpolation\n",
    "    if P < 3 or P >= len_buffer:\n",
    "        linear_interpolate_audio(buffer, len_buffer, first_bad, num_bad)\n",
    "        return\n",
    "\n",
    "    # Add a small amount of random noise to the input signal\n",
    "    # this sounds like a bad idea, but the amount we're adding\n",
    "    # is only about 1 bit in 16-bit audio, and it's an extremely\n",
    "    # effective way to avoid nearly-singular matrices.  If users\n",
    "    # run it more than once they get slightly different results;\n",
    "    # this is sometimes even advantageous.\n",
    "    s += np.random.randn(len_buffer) / 10000.0\n",
    "\n",
    "    # Solve for the best autoregression coefficients using least-squares fit \n",
    "    # to all of the non-bad\n",
    "    X = np.zeros((P, P))\n",
    "    b = np.zeros(P)\n",
    "\n",
    "    # Create matrices used to solve LS function\n",
    "    # Note, this iterates through the entire input buffer, so \n",
    "    # make sure to limit this input appropiately!\n",
    "    for i in range(len_buffer - P):\n",
    "        if i + P < first_bad or i >= (first_bad + num_bad):\n",
    "            X += np.outer(s[i:i+P], s[i:i+P])\n",
    "            b += s[i+P] * s[i:i+P]\n",
    "\n",
    "    \n",
    "    Xinv = np.linalg.inv(X)\n",
    "    if np.isnan(Xinv).any():\n",
    "        # The matrix is singular!  Fall back on linear...\n",
    "        # In practice I have never seen this happen if\n",
    "        # we add the tiny bit of random noise.\n",
    "        linear_interpolate_audio(buffer, len_buffer, first_bad, num_bad)\n",
    "        return\n",
    "\n",
    "    a = np.dot(Xinv, b)\n",
    "\n",
    "    # Create the autoregressive matrix\n",
    "    A = np.zeros((len_buffer - P, len_buffer))\n",
    "    for row in range(len_buffer - P):\n",
    "        A[row, row:row+P] = -a\n",
    "        A[row, row+P] = 1\n",
    "\n",
    "    # Split the autoregressive matrix and the signal\n",
    "    # \"u\" is for unknown (bad)\n",
    "    # \"k\" is for known (good)\n",
    "    Au = A[:, first_bad:first_bad+num_bad]\n",
    "    A_left = A[:, :first_bad]\n",
    "    A_right = A[:, first_bad+num_bad:]\n",
    "    Ak = np.concatenate((A_left, A_right), axis=1)\n",
    "    \n",
    "    s_left = s[:first_bad]\n",
    "    s_right = s[first_bad+num_bad:]\n",
    "    sk = np.concatenate((s_left, s_right))\n",
    "\n",
    "    # Do some linear algebra to find the best possible\n",
    "    # values that fill in the \"bad\" area\n",
    "    AuT = np.transpose(Au)\n",
    "    X1 = np.dot(AuT, Au)\n",
    "    try:\n",
    "        X2 = np.linalg.inv(X1)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # The matrix is singular! Fall back on linear...\n",
    "        linear_interpolate_audio(buffer, len_buffer, first_bad, num_bad)\n",
    "        return\n",
    "\n",
    "    X2b = X2 * -1.0\n",
    "    X3 = np.dot(X2b, AuT)\n",
    "    X4 = np.dot(X3, Ak)\n",
    "    # This vector contains our best guess as to the\n",
    "    # unknown values\n",
    "    su = np.dot(X4, sk)\n",
    "    \n",
    "    # Update the buffer with the interpolated values\n",
    "    s = buffer\n",
    "    s[first_bad:first_bad+num_bad] = su\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670f529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154fa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1, sr = librosa.load('{}\\WAV\\\\test 7s.wav'.format(nb_path), sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbaea377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330972"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7154079a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters for test section to fix\n",
    "bad_start_t = 0.234467\n",
    "bad_end_t = 0.240453514\n",
    "bad_start_s = int(bad_start_t * sr)\n",
    "bad_end_s = int(bad_end_t * sr)\n",
    "num_bad = bad_end_s - bad_start_s\n",
    "num_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81453e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get subsection of test1 that is the bad window, plus the length of the bad window + 50 either way\n",
    "bad_plus_buffer = test1[(bad_start_s - num_bad -50):(bad_end_s + num_bad + 50)]\n",
    "len(bad_plus_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1373dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.DataFrame(bad_plus_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6cd3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.rename(columns={0: \"Base\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bedf0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['base'] = test1[(bad_start_s - num_bad -50):(bad_end_s + num_bad + 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "549f0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0010681152\n",
      "314\n",
      "-0.0013182430164642499\n",
      "-0.001318243\n"
     ]
    }
   ],
   "source": [
    "outputs['Redone'] = interpolate_audio(bad_plus_buffer, num_bad + 50, num_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f685e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base</th>\n",
       "      <th>base</th>\n",
       "      <th>Redone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>-0.001129</td>\n",
       "      <td>-0.001129</td>\n",
       "      <td>-0.001129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-0.000961</td>\n",
       "      <td>-0.000961</td>\n",
       "      <td>-0.000961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>-0.001434</td>\n",
       "      <td>-0.001434</td>\n",
       "      <td>-0.001434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>-0.001068</td>\n",
       "      <td>-0.001068</td>\n",
       "      <td>-0.001068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>-0.001318</td>\n",
       "      <td>-0.001068</td>\n",
       "      <td>-0.001318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>-0.001286</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>-0.001451</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.001451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>-0.001372</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>-0.001372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-0.001431</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>-0.001431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>-0.001486</td>\n",
       "      <td>-0.001160</td>\n",
       "      <td>-0.001486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Base      base    Redone\n",
       "310 -0.001129 -0.001129 -0.001129\n",
       "311 -0.000961 -0.000961 -0.000961\n",
       "312 -0.001434 -0.001434 -0.001434\n",
       "313 -0.001068 -0.001068 -0.001068\n",
       "314 -0.001318 -0.001068 -0.001318\n",
       "315 -0.001286 -0.000641 -0.001286\n",
       "316 -0.001451 -0.000977 -0.001451\n",
       "317 -0.001372 -0.001099 -0.001372\n",
       "318 -0.001431 -0.000793 -0.001431\n",
       "319 -0.001486 -0.001160 -0.001486"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[310:320]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1232b3",
   "metadata": {},
   "source": [
    "Redone seems to be modifying the input array in place, which seems... bad? I know that's in the function definition but it's bloody annoying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1efe9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_csv('{}\\hope.csv'.format(nb_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b04b1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gross_mag_in_threshold(chunk, sr=44100, band_start=5000, band_end=10000):\n",
    "    \"\"\"Calculate the total intensity of frequencies within a chunk using Fourier Transform.\n",
    "    chunk - array of longs - a librosa loaded sound object containing sound data to be analyzed\n",
    "    band_start - int - the minimum frequency band to include\n",
    "    band_end - int - the maximum frequency band to include\n",
    "    \"\"\"\n",
    "    stft = librosa.stft(chunk, n_fft=len(chunk))\n",
    "    \n",
    "    # Get the frequency bin indices\n",
    "    freq_bins = librosa.fft_frequencies(sr=sr, n_fft=len(chunk))\n",
    "\n",
    "    # Get the magnitude of each frequency band by averaging each element returned in stft\n",
    "    magnit = np.abs(stft)\n",
    "    av_mag = np.mean(magnit, axis=1)\n",
    "    gross_mag = np.sum(av_mag * (freq_bins > band_start) * (freq_bins < band_end))\n",
    "    return gross_mag\n",
    "\n",
    "\n",
    "def tick_detector(chunks, start_times, chunk_ms=4, threshold_factor=3, band_start=5000, band_end=10000, sr=44100):\n",
    "    \"\"\"Returns a pandas DataFrame presenting timestamps within a sound file that contain ticks.\n",
    "    chunks - array of overlapping librosa loaded sound objects containing sound data to be analyzed\n",
    "    start_times - list of longs - timestamps (in seconds) of the time where each chunk begins\n",
    "    chunk_ms - length (in milliseconds) of each chunk\n",
    "    threshold_factor - long - how much a chunk has to exceed its neighbors by to indicate a likely tick\n",
    "    band_start - int - the minimum frequency band to include\n",
    "    band_end - int - the maximum frequency band to include\n",
    "    sr - int - sample rate of the chunks\n",
    "    \"\"\"\n",
    "    flags = np.zeros(len(chunks), dtype=bool) #initialize array with all False values\n",
    "    \n",
    "    chunk_size = int(sr * (chunk_ms / 1000))  # chunk size in samples\n",
    "\n",
    "    chunk_mags = np.array([gross_mag_in_threshold(chunk, band_start=band_start, band_end=band_end, sr=sr)\n",
    "                                   for chunk in chunks])\n",
    "    \n",
    "    # set array for comparable chunks (e.g. chunk n can only be compared to n-4 and n+4)\n",
    "    prev_chunks = chunks[:-(2 * chunk_ms)]\n",
    "    current_chunks = chunks[chunk_ms:-chunk_ms]\n",
    "    next_chunks = chunks[(2 * chunk_ms):]\n",
    "\n",
    "    prev_frequency_mag = chunk_mags[:-(2 * chunk_ms)]\n",
    "    current_frequency_mag = chunk_mags[chunk_ms:-chunk_ms]\n",
    "    next_frequency_mag = chunk_mags[(2 * chunk_ms):]\n",
    "    \n",
    "    flags[chunk_ms:-chunk_ms] = (current_frequency_mag > threshold_factor * prev_frequency_mag) & \\\n",
    "                                (current_frequency_mag > threshold_factor * next_frequency_mag)\n",
    "\n",
    "    df = pd.DataFrame({'Start Time': start_times, 'Flag': flags})\n",
    "    \n",
    "    # Filter the flagged chunks\n",
    "    mouth_sounds = df.loc[df['Flag'] == True].copy()\n",
    "    # Calculate the end times - need to use chunk_size because of rounding issues\n",
    "    mouth_sounds['End Time'] = mouth_sounds['Start Time'] + (chunk_size / sr)\n",
    "    # Reset the index\n",
    "    mouth_sounds.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return mouth_sounds\n",
    "\n",
    "\n",
    "def generate_mouth_sounds_labels(filepath, output_as = 'df', output_file_name = 'output_file.txt', sample_rate=None):\n",
    "    '''\n",
    "    Analyses a given input soundfile for potential mouthsounds and returns a table of start and end times of \n",
    "    identified bad sections. By default this is returned as a pandas dataframe for further processing by other\n",
    "    functions, but can also generate a tab delimited text file for import into Audacity.\n",
    "    filepath - string - Path to the file to be analysed\n",
    "    output_as - string - Will output as a text file (instead of a dataframe) if specified as \"labels\"\n",
    "    output_file_name - string - Name of the file to be created if output_as is set to \"labels\"\n",
    "    sample_rate - int - The specified sample rate to use in analysing the input file. \n",
    "                        If left blank it will use the file's native rate.\n",
    "    '''\n",
    "    kickoff_time = time.time()\n",
    "    print(time.strftime('%X %x %Z'))\n",
    "    \n",
    "    test1, sr = librosa.load(filepath, sr=sample_rate)\n",
    "    \n",
    "    # cutting it into chunks of \"chunk_ms\" length each starting 1ms after the other\n",
    "    chunk_ms = 4\n",
    "    chunk_size = int(sr * (chunk_ms / 1000))\n",
    "    hop_size = int(sr * 0.001)\n",
    "    \n",
    "    chunks = np.array([test1[i:i+chunk_size] for i in range(0, len(test1) - chunk_size, hop_size)])\n",
    "    start_times = np.arange(0, len(chunks) * hop_size / sr, hop_size / sr)\n",
    "\n",
    "    df_2 = tick_detector(chunks, start_times, threshold_factor=2, band_end=12000, sr=sr)\n",
    "    \n",
    "    # Initialize the consolidated dataframe\n",
    "    consolidated_chunks = pd.DataFrame(columns=['Start Time', 'End Time'])\n",
    "    \n",
    "    # Initialize variables for tracking the current chunk\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "\n",
    "    # Iterate through each row in the dataframe\n",
    "    for index, row in df_2.iterrows():\n",
    "        if current_start is None:\n",
    "            # If it's the first chunk, set the current chunk\n",
    "            current_start = row['Start Time']\n",
    "            current_end = row['End Time']\n",
    "        elif row['Start Time'] <= current_end:\n",
    "            # If the current chunk overlaps with the next chunk, extend the current chunk\n",
    "            current_end = row['End Time']\n",
    "        else:\n",
    "            # If the next chunk is not overlapping, add the consolidated chunk to the dataframe\n",
    "            consolidated_chunks = consolidated_chunks.append({'Start Time': current_start, 'End Time': current_end},\n",
    "                                                             ignore_index=True)\n",
    "            # Set the next chunk as the current chunk\n",
    "            current_start = row['Start Time']\n",
    "            current_end = row['End Time']\n",
    "\n",
    "    # Add the last consolidated chunk to the dataframe\n",
    "    consolidated_chunks = consolidated_chunks.append({'Start Time': current_start, 'End Time': current_end},\n",
    "                                                     ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # iterate over each chunk 4 steps each delayed 0.5ms to find the maximum energy in high frequency bands to narrow\n",
    "    # down the starting point. This part is ripe for optimisation. I had 1 attempt but it didn't really work out.\n",
    "    \n",
    "    # Iterate over each row in the dataframe\n",
    "    for index, row in consolidated_chunks.iterrows():\n",
    "        start_time = row['Start Time']\n",
    "        end_time = row['End Time']\n",
    "\n",
    "        # Find the corresponding samples for the start and end times\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "\n",
    "        # Extract the chunk from the original loaded .wav object\n",
    "        chunk = test1[start_sample:end_sample]\n",
    "\n",
    "        # Create four new chunks with staggered start times (0.5ms apart)\n",
    "        chunk_offsets = [0, int(0.5e-3 * sr), int(1e-3 * sr), int(1.5e-3 * sr)]\n",
    "        chunks = [chunk[offset:] for offset in chunk_offsets]\n",
    "\n",
    "        # Calculate the energy for each chunk within the desired frequency band\n",
    "        energies = [gross_mag_in_threshold(chunk, sr=sr) for chunk in chunks]\n",
    "\n",
    "        # Find the index of the chunk with the maximum energy\n",
    "        max_energy_index = max(0, np.argmax(energies)-1)\n",
    "\n",
    "        # Update the start time based on the selected chunk\n",
    "        new_start_time = start_time + chunk_offsets[max_energy_index] / sr\n",
    "\n",
    "        # Update the dataframe with the new start time\n",
    "        consolidated_chunks.loc[index, 'Start Time'] = new_start_time\n",
    "    \n",
    "    if output_as == 'labels':\n",
    "        #export the dataframe to a tab delimited text file for import into Audacity\n",
    "        consolidated_chunks['Index'] = consolidated_chunks.index\n",
    "\n",
    "        consolidated_chunks.to_csv(output_file_name, sep='\\t', columns=['Start Time', 'End Time', 'Index'], index=False)\n",
    "        print(time.strftime('%X %x %Z'))\n",
    "        print(\"time taken: --- %s seconds ---\" % (time.time() - kickoff_time))\n",
    "        return\n",
    "    \n",
    "    print(time.strftime('%X %x %Z'))\n",
    "    print(\"time taken: --- %s seconds ---\" % (time.time() - kickoff_time))\n",
    "    return consolidated_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca3d8c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_subsection(audio_samples, start_time, end_time, sample_rate, sub_buffer_length):\n",
    "    # Load the audio file\n",
    "    audio = audio_samples\n",
    "    sr = sample_rate\n",
    "\n",
    "    # Calculate the start and end indices of the subsection\n",
    "    start_index = int(start_time * sample_rate)\n",
    "    end_index = int(end_time * sample_rate)\n",
    "\n",
    "    # Determine the additional samples to include on either side\n",
    "    additional_samples = int(sub_buffer_length * sample_rate)\n",
    "\n",
    "    # Define the subsection boundaries\n",
    "    sub_start_index = max(0, start_index - additional_samples)\n",
    "    sub_end_index = min(len(audio), end_index + additional_samples)\n",
    "\n",
    "    # Extract the subsection from the audio\n",
    "    sub_buffer = audio[sub_start_index:sub_end_index]\n",
    "\n",
    "    # Calculate the indices of the bad samples within the subsection\n",
    "    first_bad = start_index - sub_start_index\n",
    "    num_bad = end_index - start_index\n",
    "\n",
    "    # Interpolate the subsection using the LSAR algorithm\n",
    "    interpolate_audio(sub_buffer, first_bad, num_bad)\n",
    "\n",
    "    # Replace the repaired subsection in the original audio buffer\n",
    "    audio[start_index:end_index] = sub_buffer[first_bad:first_bad + num_bad]\n",
    "\n",
    "    # Return the modified audio buffer\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08e5b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_repaired_wav_file(input_array, sample_rate, output_file):\n",
    "    # Normalize the input array to the range [-1, 1]\n",
    "    normalized_array = input_array / np.max(np.abs(input_array))\n",
    "    \n",
    "    # Write the repaired audio to a new .wav file\n",
    "    sf.write(output_file, normalized_array, sample_rate, 'PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ffa7483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:27:18 07/06/23 AUS Eastern Standard Time\n",
      "10:27:33 07/06/23 AUS Eastern Standard Time\n",
      "time taken: --- 14.356364965438843 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.234467</td>\n",
       "      <td>0.240454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.381134</td>\n",
       "      <td>0.386122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.398095</td>\n",
       "      <td>0.403084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.878730</td>\n",
       "      <td>1.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.159093</td>\n",
       "      <td>2.164082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Start Time  End Time\n",
       "0    0.234467  0.240454\n",
       "1    0.381134  0.386122\n",
       "2    0.398095  0.403084\n",
       "3    1.878730  1.885714\n",
       "4    2.159093  2.164082"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get bad sections\n",
    "repair_sections = generate_mouth_sounds_labels('{}\\WAV\\\\test 30s.wav'.format(nb_path))\n",
    "repair_sections.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "037384b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through bad sections to create a \"fixed\" version of the soundfile values\n",
    "file_to_fix, sr = librosa.load('{}\\WAV\\\\test 30s.wav'.format(nb_path), sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d17e4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_bad_sections(original_array, sample_rate, bad_sections_df):\n",
    "    repaired_array = original_array.copy()\n",
    "    \n",
    "    # Iterate through each row in the dataframe\n",
    "    for _, row in bad_sections_df.iterrows():\n",
    "        start_time = row['Start Time']\n",
    "        end_time = row['End Time']\n",
    "        \n",
    "        # Convert start and end times to sample indices\n",
    "        start_index = int(start_time * sample_rate)\n",
    "        end_index = int(end_time * sample_rate)\n",
    "        \n",
    "        # Repair the bad section using your interpolation method\n",
    "        repaired_section = interpolate_subsection(original_array, start_time, end_time, sample_rate, \n",
    "                                                  sub_buffer_length=(end_time - start_time)*1.5)\n",
    "        \n",
    "        # Replace the bad section in the repaired array\n",
    "        repaired_array = repaired_section\n",
    "    \n",
    "    return repaired_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14dc4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_file_samples = repair_bad_sections(file_to_fix, sr, repair_sections)\n",
    "create_repaired_wav_file(fixed_file_samples, sr, '{}\\WAV\\\\test 30s - repaired.wav'.format(nb_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e42891",
   "metadata": {},
   "source": [
    "Hmmm. This works, but there are a couple of missed spots and weird mic blowouts that I don't remember being there. I need to compare the actual soundwaves visually for myself, so I'll check those out in Audacity when I get some time. Yes, I realise librosa can 100% do this for me, but no surprise the a GUI is superior for ease of access and comparing where the corrections did vs didn't work. It may be that the sub-sections are too long for LSAR to work effectively, so I'd need to use the approach I adopted in Audacity itself which was to repair 128 samples at a time, with 64 sample step forwards to cover the full length of the bad section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06692bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
